{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-xx : Large Scale Machine Learning\n",
    "Problem Identification : Initially we load the entire dataset onto our secondary memory drive and then use it train our models but for large datasets(Big data) practically it's not possible to always load complete data onto our memory then preprocess and use it for training of our models.\n",
    "\n",
    "Thus We learn how to handle large scale datasets, perform preprocessing and using it for training of our machine learning models via incremental learning. \n",
    "\n",
    "Hence we need a strategies to scale computationally that can handle large amount of data i.e Big Data. Processing big data are challenging task for the traditional machine learning approaches.\n",
    "\n",
    "**What do we mean by scaling with instance using out of core learning ?**\n",
    "> Out of core(external) learning is a technique, which is used to learn from the data that cannot fit in a computer’s main memory (RAM).\n",
    "Scikit-learn : system designed to achieve this goal:\n",
    "1. a way to stream instances\n",
    "2. a way to extract features from instances\n",
    "3. an incremental algorithm\n",
    "\n",
    "**What do we mean by Stream instances ?** > > a reader that yields instances from files on a hard drive, a database, from a network stream etc.\n",
    "\n",
    "**What do we mean by the extrating features ?**\n",
    "> Extracting features could be any relevant way to extract features among the different feature extraction methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care.\n",
    "\n",
    "Example-1 :  text classification where unknown terms are likely to be found during training. It is possible to use a statefull vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called hashing trick as implemented by sklearn.`feature_extraction.FeatureHasher` for datasets with categorical variables represented as list of Python dicts or `sklearn.feature_extraction.text.HashingVectorizer` for text documents.\n",
    "\n",
    "\n",
    "**What do we mean by incremental learning ?**\n",
    "> all algorithms cannot learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the partial_fit API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called “online learning”) is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning[1](https://scikit-learn.org/0.15/modules/scaling_strategies.html#id2)\n",
    "\n",
    "List of incremental estimators for different tasks\n",
    "1. Classification\n",
    "   - sklearn.naive_bayes.MultinomialNB\n",
    "   - sklearn.naive_bayes.BernoulliNB\n",
    "   - sklearn.linear_model.Perceptron\n",
    "   - sklearn.linear_model.SGDClassifier\n",
    "   - sklearn.linear_model.PassiveAggressiveClassifier\n",
    "2. Regression\n",
    "   - sklearn.linear_model.SGDRegressor\n",
    "   - sklearn.linear_model.PassiveAggressiveRegressor\n",
    "3. Clustering\n",
    "   - sklearn.cluster.MiniBatchKMeans\n",
    "4. Feature Extraction(Decomposition)\n",
    "   - sklearn.decomposition.\n",
    "   - MiniBatchDictionaryLearning\n",
    "sklearn.cluster.MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-0 : Setup development enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn module's\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-1 : Traditional ML approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make synthetic dataset for the classification task\n",
    "X, y = make_classification(\n",
    "    n_samples=40000, \n",
    "    n_features=10,\n",
    "    n_clusters_per_class=1\n",
    ")\n",
    "# split the classification dataset into the training and testing with a fraction of 0.2\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# training SGDClassifier\n",
    "classifier1 = SGDClassifier(\n",
    "    max_iter=1000,\n",
    "    tol=0.01\n",
    ")\n",
    "classifier1.fit(X_train, Y_train)\n",
    "\n",
    "# let's see the score of the classifier model\n",
    "trainingScore  = classifier1.score(X_train,\n",
    "                                   Y_train)\n",
    "print(\"Training Score of SGDClassifier : \",trainingScore)\n",
    "\n",
    "# Testing score of classifier1\n",
    "testingScore = classifier1.score(X_test, Y_test)\n",
    "print(\"Testing score of the classifier\")\n",
    "\n",
    "# prediction of the classifer1 model\n",
    "Y_pred = classifier1.predict(X_test)\n",
    "\n",
    "# Let's see the classification report of the classifier\n",
    "classfier-report = classification_report(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-3 : Incremental Learning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
